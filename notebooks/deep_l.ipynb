{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "df_prices=pd.read_csv('data_deep_learning/open_prices.csv', index_col=0)\n",
    "targets = np.load('data_deep_learning/targets', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "cols = list(df_prices.columns)\n",
    "names = [\"BTC/USDT\",\"NEIRO/USDT\",\"1000PEPE/USDT\",\"ETH/USDT\",\"REEF/USDT\",\"SOL/USDT\",\"NEIROETH/USDT\",\"HMSTR/USDT\",\"CATI/USDT\",\"UXLINK/USDT\",\"DOGS/USDT\",\"1MBABYDOGE/USDT\",\"1000SHIB/USDT\",\"BIGTIME/USDT\",\"POPCAT/USDT\",\"BNB/USDT\",\"SUI/USDT\",\"WIF/USDT\",\"XRP/USDT\",\"1000FLOKI/USDT\",\"1000BONK/USDT\",\"TON/USDT\",\"TURBO/USDT\",\"1000RATS/USDT\",\"MEW/USDT\",\"FET/USDT\",\"DOGE/USDT\",\"1000SATS/USDT\",\"ZETA/USDT\",\"ONDO/USDT\"]\n",
    "dic_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_f64=df_prices.notna().idxmax().sort_values()\n",
    "nans_per_i=[ sum([np.isnan(k) for k in targets[:, i]]) for i in range(targets.shape[1])]\n",
    "n_list = [(i,int(val)) for i, val in enumerate(nans_per_i)]\n",
    "andidates1=sorted(n_list, key = lambda x:x[1])[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(df_prices.columns)\n",
    "c_series=[(i, j) for i,j in andidates1 if cols[i] in series_f64[:64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"BTC/USDT\",\"NEIRO/USDT\",\"1000PEPE/USDT\",\"ETH/USDT\",\"REEF/USDT\",\"SOL/USDT\",\"NEIROETH/USDT\",\"HMSTR/USDT\",\"CATI/USDT\",\"UXLINK/USDT\",\"DOGS/USDT\",\"1MBABYDOGE/USDT\",\"1000SHIB/USDT\",\"BIGTIME/USDT\",\"POPCAT/USDT\",\"BNB/USDT\",\"SUI/USDT\",\"WIF/USDT\",\"XRP/USDT\",\"1000FLOKI/USDT\",\"1000BONK/USDT\",\"TON/USDT\",\"TURBO/USDT\",\"1000RATS/USDT\",\"MEW/USDT\",\"FET/USDT\",\"DOGE/USDT\",\"1000SATS/USDT\",\"ZETA/USDT\",\"ONDO/USDT\"\n",
    "]\n",
    "n_names=[ v for v in enumerate(cols) if v[1] in names]\n",
    "n_dic_2 = {}\n",
    "#have prices up to the last timestamp\n",
    "for i in range(len(n_names)):\n",
    "    if np.isnan(df_prices[n_names[i][1]].iloc[-1]):\n",
    "        print(\"Miss last price at \", n_names[i][1])\n",
    "count = 0       \n",
    "#need to find where the last nan is located!! \n",
    "tolerance = 100\n",
    "for i in range(len(n_names)):\n",
    "    for j in range(1, 1000):\n",
    "        if not np.isnan(targets[-j,n_names[i][0]]):\n",
    "            add =  1\n",
    "            while  not np.isnan(targets[-(j + add),n_names[i][0]]) and add < tolerance:\n",
    "                add +=1\n",
    "            if add == tolerance:\n",
    "                n_dic_2[n_names[i][1]]={\n",
    "                    'index': n_names[i][0],\n",
    "                    'top_i': j\n",
    "                }  \n",
    "                break \n",
    "            else:\n",
    "                print( \" there are more nan's at \",n_names[i][1], \" smaller than -\",j)    \n",
    "            \n",
    "print(count)        \n",
    "##check if at those indices there are more nans\n",
    "#checking if assigning a bibber tolerance gives different top_i values, it does not, tol of 40 if fine\n",
    "[ (k, val) for k, val in n_dic.items() if val['top_i'] != n_dic_2[k]['top_i']]\n",
    "\n",
    "# for k, val in n_dic.items():\n",
    "#     for i in range(-val['top_i'], 100):\n",
    "#         if np.isnan(targets[-i,val['index']]):\n",
    "#             print(f\" Careful, something to check at {k} at index -{i}\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "# with open (\"dic_30_series.json\", \"w\") as f:\n",
    "#     json.dump(n_dic, f)\n",
    "import pickle\n",
    "with open('/Users/carlos/Documents/Peccala/deepL_tests/input_log_rets.pkl', 'rb') as file:\n",
    "   log_rets = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "path0=\"few_series/\"\n",
    "with open(path0+\"dic_np_saved_series.json\", 'r') as file:\n",
    "    dic_s = json.load(file) \n",
    "len(dic_s)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1000BONK/USDT', '1000FLOKI/USDT', '1000PEPE/USDT', '1000RATS/USDT', '1000SATS/USDT', '1000SHIB/USDT', 'BIGTIME/USDT', 'BNB/USDT', 'BTC/USDT', 'DOGE/USDT', 'ETH/USDT', 'FET/USDT', 'MEW/USDT', 'ONDO/USDT', 'REEF/USDT', 'SOL/USDT', 'SUI/USDT', 'TURBO/USDT', 'WIF/USDT', 'XRP/USDT', 'ZETA/USDT'])\n"
     ]
    }
   ],
   "source": [
    "dic ={\"1000BONK/USDT\": {\"index\": 1, \"top_i\": 11}, \"1000FLOKI/USDT\": {\"index\": 5, \"top_i\": 13}, \"1000PEPE/USDT\": {\"index\": 7, \"top_i\": 12}, \"1000RATS/USDT\": {\"index\": 8, \"top_i\": 8}, \"1000SATS/USDT\": {\"index\": 9, \"top_i\": 13}, \"1000SHIB/USDT\": {\"index\": 10, \"top_i\": 15}, \"BIGTIME/USDT\": {\"index\": 65, \"top_i\": 13}, \"BNB/USDT\": {\"index\": 68, \"top_i\": 15}, \"BTC/USDT\": {\"index\": 77, \"top_i\": 30}, \"DOGE/USDT\": {\"index\": 113, \"top_i\": 13}, \"ETH/USDT\": {\"index\": 130, \"top_i\": 125}, \"FET/USDT\": {\"index\": 131, \"top_i\": 13}, \"MEW/USDT\": {\"index\": 214, \"top_i\": 25}, \"ONDO/USDT\": {\"index\": 237, \"top_i\": 12}, \"REEF/USDT\": {\"index\": 265, \"top_i\": 13}, \"SOL/USDT\": {\"index\": 293, \"top_i\": 15}, \"SUI/USDT\": {\"index\": 305, \"top_i\": 7}, \"TURBO/USDT\": {\"index\": 325, \"top_i\": 12}, \"WIF/USDT\": {\"index\": 339, \"top_i\": 12}, \"XRP/USDT\": {\"index\": 347, \"top_i\": 9}, \"ZETA/USDT\": {\"index\": 356, \"top_i\": 13}}\n",
    "print(dic.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('/Users/carlos/Documents/Peccala/deepL_tests/deep_learning/input_histograms.pkl', 'rb') as file:\n",
    "#    histograms = pickle.load(file)\n",
    "hist_2_keep = { key: histograms[key] for key in dic_s}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5875"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for key in hist_2_keep:\n",
    "#     k=key\n",
    "#     arr_ex = hist_2_keep[key][-6000 : -dic_s[k]['top_i']]\n",
    "#     hist_2_keep[key] = arr_ex\n",
    "# import numpy as np\n",
    "# np_aux = np.load('/Users/carlos/Documents/Peccala/deepL_tests/few_series/np_series_concat.npy')\n",
    "np_aux.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12  has nans\n",
      "13  has nans\n",
      "17  has nans\n",
      "18  has nans\n",
      "20  has nans\n"
     ]
    }
   ],
   "source": [
    "# np_aux_f=np.array([np.hstack([hist_2_keep[val][-np_aux.shape[1]:], np_aux[i,:,:]]) for i, val in enumerate(hist_2_keep.keys())])\n",
    "# np_aux_f.shape\n",
    "#np.save('/Users/carlos/Documents/Peccala/deepL_tests/few_series/np_series_concat2.npy', np_aux_f)\n",
    "for i in range(int(np_aux_f.shape[0])):\n",
    "    x = np_aux_f[i,:,:]\n",
    "    if np.isnan(x).any().any():\n",
    "        print(i , \" has nans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5989, 72)\n",
      "(5987, 72)\n",
      "(5988, 72)\n",
      "(5992, 72)\n",
      "(5987, 72)\n",
      "(5985, 72)\n",
      "(5987, 72)\n",
      "(5985, 72)\n",
      "(5970, 72)\n",
      "(5987, 72)\n",
      "(5875, 72)\n",
      "(5987, 72)\n",
      "(5975, 72)\n",
      "(5988, 72)\n",
      "(5987, 72)\n",
      "(5985, 72)\n",
      "(5993, 72)\n",
      "(5988, 72)\n",
      "(5988, 72)\n",
      "(5991, 72)\n",
      "(5987, 72)\n"
     ]
    }
   ],
   "source": [
    "for k, val in hist_2_keep.items():\n",
    "    print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1MBABYDOGE/USDT',\n",
       " 'CATI/USDT',\n",
       " 'DOGS/USDT',\n",
       " 'HMSTR/USDT',\n",
       " 'NEIROETH/USDT',\n",
       " 'NEIRO/USDT',\n",
       " 'POPCAT/USDT',\n",
       " 'TON/USDT',\n",
       " 'UXLINK/USDT']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_dic={\"1000BONK/USDT\": {\"index\": 1, \"top_i\": 11}, \"1000FLOKI/USDT\": {\"index\": 5, \"top_i\": 13}, \"1000PEPE/USDT\": {\"index\": 7, \"top_i\": 12}, \"1000RATS/USDT\": {\"index\": 8, \"top_i\": 8}, \"1000SATS/USDT\": {\"index\": 9, \"top_i\": 13}, \"1000SHIB/USDT\": {\"index\": 10, \"top_i\": 15}, \"1MBABYDOGE/USDT\": {\"index\": 15, \"top_i\": 12}, \"BIGTIME/USDT\": {\"index\": 65, \"top_i\": 13}, \"BNB/USDT\": {\"index\": 68, \"top_i\": 15}, \"BTC/USDT\": {\"index\": 77, \"top_i\": 30}, \"CATI/USDT\": {\"index\": 83, \"top_i\": 13}, \"DOGE/USDT\": {\"index\": 113, \"top_i\": 13}, \"DOGS/USDT\": {\"index\": 114, \"top_i\": 13}, \"ETH/USDT\": {\"index\": 130, \"top_i\": 125}, \"FET/USDT\": {\"index\": 131, \"top_i\": 13}, \"HMSTR/USDT\": {\"index\": 160, \"top_i\": 13}, \"MEW/USDT\": {\"index\": 214, \"top_i\": 25}, \"NEIROETH/USDT\": {\"index\": 222, \"top_i\": 13}, \"NEIRO/USDT\": {\"index\": 223, \"top_i\": 13}, \"ONDO/USDT\": {\"index\": 237, \"top_i\": 12}, \"POPCAT/USDT\": {\"index\": 254, \"top_i\": 13}, \"REEF/USDT\": {\"index\": 265, \"top_i\": 13}, \"SOL/USDT\": {\"index\": 293, \"top_i\": 15}, \"SUI/USDT\": {\"index\": 305, \"top_i\": 7}, \"TON/USDT\": {\"index\": 320, \"top_i\": 13}, \"TURBO/USDT\": {\"index\": 325, \"top_i\": 12}, \"UXLINK/USDT\": {\"index\": 332, \"top_i\": 13}, \"WIF/USDT\": {\"index\": 339, \"top_i\": 12}, \"XRP/USDT\": {\"index\": 347, \"top_i\": 9}, \"ZETA/USDT\": {\"index\": 356, \"top_i\": 13}}\n",
    "#np_rets = [log_rets[key][-6000 :-n_dic[key][\"top_i\"]] for key in n_dic ]\n",
    "[k for i, k in enumerate(n_dic.keys()) if i in [6, 10 ,12, 15, 17, 18, 20, 24, 26]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for j, arr in enumerate(np_rets):\n",
    "#     for i,k in enumerate(arr):\n",
    "#         if i> len(arr) - 3032 and np.isnan(k).any():\n",
    "#             print(\"Nans at \", i, \" in item \", j)\n",
    "#             break\n",
    "# forbid =  ['1MBABYDOGE/USDT','CATI/USDT','DOGS/USDT','HMSTR/USDT','NEIROETH/USDT','NEIRO/USDT','POPCAT/USDT','TON/USDT','UXLINK/USDT']       \n",
    "# n_dic={\"1000BONK/USDT\": {\"index\": 1, \"top_i\": 11}, \"1000FLOKI/USDT\": {\"index\": 5, \"top_i\": 13}, \"1000PEPE/USDT\": {\"index\": 7, \"top_i\": 12}, \"1000RATS/USDT\": {\"index\": 8, \"top_i\": 8}, \"1000SATS/USDT\": {\"index\": 9, \"top_i\": 13}, \"1000SHIB/USDT\": {\"index\": 10, \"top_i\": 15}, \"1MBABYDOGE/USDT\": {\"index\": 15, \"top_i\": 12}, \"BIGTIME/USDT\": {\"index\": 65, \"top_i\": 13}, \"BNB/USDT\": {\"index\": 68, \"top_i\": 15}, \"BTC/USDT\": {\"index\": 77, \"top_i\": 30}, \"CATI/USDT\": {\"index\": 83, \"top_i\": 13}, \"DOGE/USDT\": {\"index\": 113, \"top_i\": 13}, \"DOGS/USDT\": {\"index\": 114, \"top_i\": 13}, \"ETH/USDT\": {\"index\": 130, \"top_i\": 125}, \"FET/USDT\": {\"index\": 131, \"top_i\": 13}, \"HMSTR/USDT\": {\"index\": 160, \"top_i\": 13}, \"MEW/USDT\": {\"index\": 214, \"top_i\": 25}, \"NEIROETH/USDT\": {\"index\": 222, \"top_i\": 13}, \"NEIRO/USDT\": {\"index\": 223, \"top_i\": 13}, \"ONDO/USDT\": {\"index\": 237, \"top_i\": 12}, \"POPCAT/USDT\": {\"index\": 254, \"top_i\": 13}, \"REEF/USDT\": {\"index\": 265, \"top_i\": 13}, \"SOL/USDT\": {\"index\": 293, \"top_i\": 15}, \"SUI/USDT\": {\"index\": 305, \"top_i\": 7}, \"TON/USDT\": {\"index\": 320, \"top_i\": 13}, \"TURBO/USDT\": {\"index\": 325, \"top_i\": 12}, \"UXLINK/USDT\": {\"index\": 332, \"top_i\": 13}, \"WIF/USDT\": {\"index\": 339, \"top_i\": 12}, \"XRP/USDT\": {\"index\": 347, \"top_i\": 9}, \"ZETA/USDT\": {\"index\": 356, \"top_i\": 13}}\n",
    "# n_dic = {k : n_dic[k] for k in n_dic if not k in forbid} \n",
    "n_arr = [k for i, k in enumerate(np_rets) if i not in [6, 10 ,12, 15, 17, 18, 20, 24, 26]]\n",
    "len(n_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_arr[0][0].shape\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[-(n_dic['1000BONK/USDT']['top_i'] -1), n_dic['1000BONK/USDT']['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = [(i,int(val)) for i, val in enumerate(nans_per_i)]\n",
    "len(names)\n",
    "dic_s_i= {}\n",
    "for i,_ in c_series:\n",
    "\n",
    "    key = cols[i]\n",
    "    dic_s_i[key]={\"i_target\":i}\n",
    "    aux_series = targets[:,i]\n",
    "    l_i=0\n",
    "    for j, val in enumerate(aux_series):\n",
    "        if not np.isnan(val) and not np.isnan(df_prices[key].iloc[j]):\n",
    "            dic_s_i[key][\"lower_target_i\"]=j\n",
    "            l_i=j\n",
    "            break\n",
    "    for j in range(aux_series.shape[0] -1, 0, -1) :\n",
    "        if not np.isnan(sum(targets[l_i:j+1, i]))  and not np.isnan(df_prices[key].iloc[j]):\n",
    "            dic_s_i[key]['upper_target_i']=j\n",
    "            break\n",
    "    \n",
    "\n",
    "#dic_s_i is the dictionary to index the series with the corresp. position in the target np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(n_dic.keys()):\n",
    "    target_s = targets[:-n_dic[key][\"top_i\"], n_dic[key][\"index\"]]\n",
    "    prices_s = df_prices[key][:-n_dic[key][\"top_i\"]]\n",
    "    target_s=target_s[-n_arr[i].shape[0]:]\n",
    "    prices_s=prices_s[-n_arr[i].shape[0]:]\n",
    "    # if target_s.shape[0] != n_arr[i].shape[0]:\n",
    "    #     print(\"mismatch target at \", key)\n",
    "    # if prices_s.shape[0] != n_arr[i].shape[0]:\n",
    "    #     print(\"mismatch price shape at \", key) \n",
    "    resulting_series = np.array([np.hstack([orig, add1, add2]) for orig, add1, add2 in zip(n_arr[i], prices_s, target_s)])\n",
    "\n",
    "# Check the shape of the first element\n",
    "   \n",
    "    n_arr[i]=resulting_series\n",
    "   \n",
    "    # print(f\"for key {key}\")\n",
    "    # print(f\" have corresp shapes {target_s.shape} and {prices_s.shape} \")\n",
    "    # print(f\" and types {type(target_s)} and {type(prices_s)}\")\n",
    "    # print(f\"the one to match has shape {n_arr[0].shape}\")\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_dic)-np_arr_series.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_arr_series = np.array([n[-5875:,:] for n in n_arr])\n",
    "#np_arr_series.shape\n",
    "import json\n",
    "with open (\"few_series/dic_np_saved_series.json\", \"w\") as f:\n",
    "    json.dump(n_dic, f)\n",
    "np.save(\"few_series/np_series_concat\", np_arr_series)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "dict_keys(['1000BONK/USDT', '1000FLOKI/USDT', '1000PEPE/USDT', '1000RATS/USDT', '1000SATS/USDT', '1000SHIB/USDT', 'BIGTIME/USDT', 'BNB/USDT', 'BTC/USDT', 'DOGE/USDT', 'ETH/USDT', 'FET/USDT', 'MEW/USDT', 'ONDO/USDT', 'REEF/USDT', 'SOL/USDT', 'SUI/USDT', 'TURBO/USDT', 'WIF/USDT', 'XRP/USDT', 'ZETA/USDT'])\n"
     ]
    }
   ],
   "source": [
    "dic={\"1000BONK/USDT\": {\"index\": 1, \"top_i\": 11}, \"1000FLOKI/USDT\": {\"index\": 5, \"top_i\": 13}, \"1000PEPE/USDT\": {\"index\": 7, \"top_i\": 12}, \"1000RATS/USDT\": {\"index\": 8, \"top_i\": 8}, \"1000SATS/USDT\": {\"index\": 9, \"top_i\": 13}, \"1000SHIB/USDT\": {\"index\": 10, \"top_i\": 15}, \"BIGTIME/USDT\": {\"index\": 65, \"top_i\": 13}, \"BNB/USDT\": {\"index\": 68, \"top_i\": 15}, \"BTC/USDT\": {\"index\": 77, \"top_i\": 30}, \"DOGE/USDT\": {\"index\": 113, \"top_i\": 13}, \"ETH/USDT\": {\"index\": 130, \"top_i\": 125}, \"FET/USDT\": {\"index\": 131, \"top_i\": 13}, \"MEW/USDT\": {\"index\": 214, \"top_i\": 25}, \"ONDO/USDT\": {\"index\": 237, \"top_i\": 12}, \"REEF/USDT\": {\"index\": 265, \"top_i\": 13}, \"SOL/USDT\": {\"index\": 293, \"top_i\": 15}, \"SUI/USDT\": {\"index\": 305, \"top_i\": 7}, \"TURBO/USDT\": {\"index\": 325, \"top_i\": 12}, \"WIF/USDT\": {\"index\": 339, \"top_i\": 12}, \"XRP/USDT\": {\"index\": 347, \"top_i\": 9}, \"ZETA/USDT\": {\"index\": 356, \"top_i\": 13}}\n",
    "print(len(dic))\n",
    "\n",
    "print(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/11hy697j7mv487fs334bjwvr0000gp/T/ipykernel_4987/2455240514.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  log_rets = pickle.load(file)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/Users/carlos/Documents/Peccala/deepL_tests/input_log_rets.pkl', 'rb') as file:\n",
    "   log_rets = pickle.load(file)\n",
    "#with open('/Users/carlos/Documents/Peccala/deepL_tests/data_deep_learning/input_histograms.pkl') as file:\n",
    " #   hist = pickle.load(file)\n",
    "\n",
    "updates = 0\n",
    "for key in dic_s_i:\n",
    "   for j in range(dic_s_i[key]['lower_target_i'], len(log_rets[key])):\n",
    "      if sum([ np.isnan(k) for k in log_rets[key][j]]) == 0:\n",
    "         if dic_s_i[key]['lower_target_i'] < j:\n",
    "            dic_s_i[key]['lower_target_i']=j \n",
    "            updates +=1\n",
    "            #print(f\"an l-update at {key}\")\n",
    "         break\n",
    "   for j in range(len(log_rets[key])-1, 0, -1):\n",
    "      if sum([ np.isnan(k) for k in log_rets[key][j]]) == 0:\n",
    "         if dic_s_i[key]['upper_target_i'] >j:\n",
    "            dic_s_i[key]['upper_target_i'] =j\n",
    "            print(f\"an u-update at {key}\")\n",
    "            updates +=1\n",
    "         break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = \"data_deep_learning/series_data/\"\n",
    "for key in dic_s_i:\n",
    "    ind = dic_s_i[key]['i_target']\n",
    "    l_i = dic_s_i[key]['lower_target_i']\n",
    "    u_i = dic_s_i[key]['upper_target_i']\n",
    "    rets = log_rets[key]\n",
    "    f_name = key.replace('/','&')\n",
    "    aux_np=np.column_stack((rets[l_i:u_i, :],  df_prices[key][l_i: u_i], targets[l_i: u_i, ind]))\n",
    "    np.save(directory_path+f_name+'.npy', aux_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/11hy697j7mv487fs334bjwvr0000gp/T/ipykernel_7694/3503708678.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  hist = pickle.load(file)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/Users/carlos/Documents/Peccala/deepL_tests/data_deep_learning/input_histograms.pkl', 'rb') as file:\n",
    "    hist = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_prices\n",
    "del targets\n",
    "del log_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#with open (\"dic_series.json\", \"w\") as f:\n",
    " #   json.dump(dic_s_i, f)\n",
    "with open(\"dic_series.json\", 'r') as file:\n",
    "    dic_s = json.load(file)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the key DUSK/USDT\n",
      "old shape (46825, 74)\n",
      "n shape (46825, 72)\n",
      "result s  (46825, 146)\n",
      "for the key XMR/USDT\n",
      "old shape (49914, 74)\n",
      "n shape (49914, 72)\n",
      "result s  (49914, 146)\n",
      "for the key MATIC/USDT\n",
      "old shape (46945, 74)\n",
      "n shape (46945, 72)\n",
      "result s  (46945, 146)\n",
      "for the key HOT/USDT\n",
      "old shape (50519, 74)\n",
      "n shape (50519, 72)\n",
      "result s  (50519, 146)\n",
      "for the key DOGE/USDT\n",
      "old shape (47230, 74)\n",
      "n shape (47230, 72)\n",
      "result s  (47230, 146)\n",
      "for the key BAND/USDT\n",
      "old shape (45430, 74)\n",
      "n shape (45430, 72)\n",
      "result s  (45430, 146)\n",
      "for the key LINK/USDT\n",
      "old shape (51310, 74)\n",
      "n shape (51310, 72)\n",
      "result s  (51310, 146)\n",
      "for the key REN/USDT\n",
      "old shape (45272, 74)\n",
      "n shape (45272, 72)\n",
      "result s  (45272, 146)\n",
      "for the key WAVES/USDT\n",
      "old shape (47368, 74)\n",
      "n shape (47368, 72)\n",
      "result s  (47368, 146)\n",
      "for the key CVC/USDT\n",
      "old shape (44837, 74)\n",
      "n shape (44837, 72)\n",
      "result s  (44837, 146)\n",
      "for the key ZRX/USDT\n",
      "old shape (50297, 74)\n",
      "n shape (50297, 72)\n",
      "result s  (50297, 146)\n",
      "for the key IOTX/USDT\n",
      "old shape (44071, 74)\n",
      "n shape (44071, 72)\n",
      "result s  (44071, 146)\n",
      "for the key ONT/USDT\n",
      "old shape (56643, 74)\n",
      "n shape (56643, 72)\n",
      "result s  (56643, 146)\n",
      "for the key TOMO/USDT\n",
      "old shape (37093, 74)\n",
      "n shape (37093, 72)\n",
      "result s  (37093, 146)\n",
      "for the key CELR/USDT\n",
      "old shape (49675, 74)\n",
      "n shape (49675, 72)\n",
      "result s  (49675, 146)\n",
      "for the key BTC/USDT\n",
      "old shape (63709, 74)\n",
      "n shape (63709, 72)\n",
      "result s  (63709, 146)\n",
      "for the key DENT/USDT\n",
      "old shape (45966, 74)\n",
      "n shape (45966, 72)\n",
      "result s  (45966, 146)\n",
      "for the key RVN/USDT\n",
      "old shape (45262, 74)\n",
      "n shape (45262, 72)\n",
      "result s  (45262, 146)\n",
      "for the key THETA/USDT\n",
      "old shape (49308, 74)\n",
      "n shape (49308, 72)\n",
      "result s  (49308, 146)\n",
      "for the key FTT/USDT\n",
      "old shape (42156, 74)\n",
      "n shape (42156, 72)\n",
      "result s  (42156, 146)\n",
      "for the key IOST/USDT\n",
      "old shape (49762, 74)\n",
      "n shape (49762, 72)\n",
      "result s  (49762, 146)\n",
      "for the key NULS/USDT\n",
      "old shape (55566, 74)\n",
      "n shape (55566, 72)\n",
      "result s  (55566, 146)\n",
      "for the key ANKR/USDT\n",
      "old shape (46796, 74)\n",
      "n shape (46796, 72)\n",
      "result s  (46796, 146)\n",
      "for the key ETC/USDT\n",
      "old shape (56550, 74)\n",
      "n shape (56550, 72)\n",
      "result s  (56550, 146)\n",
      "for the key ATOM/USDT\n",
      "old shape (48844, 74)\n",
      "n shape (48844, 72)\n",
      "result s  (48844, 146)\n",
      "for the key ZEC/USDT\n",
      "old shape (49781, 74)\n",
      "n shape (49781, 72)\n",
      "result s  (49781, 146)\n",
      "for the key ETH/USDT\n",
      "old shape (63614, 74)\n",
      "n shape (63614, 72)\n",
      "result s  (63614, 146)\n",
      "for the key NEO/USDT\n",
      "old shape (61447, 74)\n",
      "n shape (61447, 72)\n",
      "result s  (61447, 146)\n",
      "for the key VET/USDT\n",
      "old shape (55522, 74)\n",
      "n shape (55522, 72)\n",
      "result s  (55522, 146)\n",
      "for the key ONG/USDT\n",
      "old shape (50572, 74)\n",
      "n shape (50572, 72)\n",
      "result s  (50572, 146)\n",
      "for the key EOS/USDT\n",
      "old shape (56910, 74)\n",
      "n shape (56910, 72)\n",
      "result s  (56910, 146)\n",
      "for the key FTM/USDT\n",
      "old shape (47818, 74)\n",
      "n shape (47818, 72)\n",
      "result s  (47818, 146)\n",
      "for the key OGN/USDT\n",
      "old shape (42727, 74)\n",
      "n shape (42727, 72)\n",
      "result s  (42727, 146)\n",
      "for the key XLM/USDT\n",
      "old shape (56838, 74)\n",
      "n shape (56838, 72)\n",
      "result s  (56838, 146)\n",
      "for the key CHZ/USDT\n",
      "old shape (45726, 74)\n",
      "n shape (45726, 72)\n",
      "result s  (45726, 146)\n",
      "for the key ENJ/USDT\n",
      "old shape (49114, 74)\n",
      "n shape (49114, 72)\n",
      "result s  (49114, 146)\n",
      "for the key IOTA/USDT\n",
      "old shape (56833, 74)\n",
      "n shape (56833, 72)\n",
      "result s  (56833, 146)\n",
      "for the key NKN/USDT\n",
      "old shape (44876, 74)\n",
      "n shape (44876, 72)\n",
      "result s  (44876, 146)\n",
      "for the key ADA/USDT\n",
      "old shape (57898, 74)\n",
      "n shape (57898, 72)\n",
      "result s  (57898, 146)\n",
      "for the key STX/USDT\n",
      "old shape (44550, 74)\n",
      "n shape (44550, 72)\n",
      "result s  (44550, 146)\n",
      "for the key QTUM/USDT\n",
      "old shape (58586, 74)\n",
      "n shape (58586, 72)\n",
      "result s  (58586, 146)\n",
      "for the key FET/USDT\n",
      "old shape (50280, 74)\n",
      "n shape (50280, 72)\n",
      "result s  (50280, 146)\n",
      "for the key ICX/USDT\n",
      "old shape (56522, 74)\n",
      "n shape (56522, 72)\n",
      "result s  (56522, 146)\n",
      "for the key HBAR/USDT\n",
      "old shape (45178, 74)\n",
      "n shape (45178, 72)\n",
      "result s  (45178, 146)\n",
      "for the key ARPA/USDT\n",
      "old shape (44252, 74)\n",
      "n shape (44252, 72)\n",
      "result s  (44252, 146)\n",
      "for the key BNB/USDT\n",
      "old shape (61781, 74)\n",
      "n shape (61781, 72)\n",
      "result s  (61781, 146)\n",
      "for the key BAT/USDT\n",
      "old shape (50184, 74)\n",
      "n shape (50184, 72)\n",
      "result s  (50184, 146)\n",
      "for the key BCH/USDT\n",
      "old shape (43717, 74)\n",
      "n shape (43717, 72)\n",
      "result s  (43717, 146)\n",
      "for the key MTL/USDT\n",
      "old shape (46056, 74)\n",
      "n shape (46056, 72)\n",
      "result s  (46056, 146)\n",
      "for the key OMG/USDT\n",
      "old shape (49471, 74)\n",
      "n shape (49471, 72)\n",
      "result s  (49471, 146)\n",
      "for the key COS/USDT\n",
      "old shape (46534, 74)\n",
      "n shape (46534, 72)\n",
      "result s  (46534, 146)\n",
      "for the key KAVA/USDT\n",
      "old shape (44552, 74)\n",
      "n shape (44552, 72)\n",
      "result s  (44552, 146)\n",
      "for the key XRP/USDT\n",
      "old shape (57486, 74)\n",
      "n shape (57486, 72)\n",
      "result s  (57486, 146)\n",
      "for the key LTC/USDT\n",
      "old shape (60884, 74)\n",
      "n shape (60884, 72)\n",
      "result s  (60884, 146)\n",
      "for the key DASH/USDT\n",
      "old shape (49614, 74)\n",
      "n shape (49614, 72)\n",
      "result s  (49614, 146)\n",
      "for the key ONE/USDT\n",
      "old shape (48054, 74)\n",
      "n shape (48054, 72)\n",
      "result s  (48054, 146)\n",
      "for the key TRX/USDT\n",
      "old shape (56565, 74)\n",
      "n shape (56565, 72)\n",
      "result s  (56565, 146)\n",
      "for the key ALGO/USDT\n",
      "old shape (47560, 74)\n",
      "n shape (47560, 72)\n",
      "result s  (47560, 146)\n",
      "for the key RLC/USDT\n",
      "old shape (44069, 74)\n",
      "n shape (44069, 72)\n",
      "result s  (44069, 146)\n",
      "for the key ZIL/USDT\n",
      "old shape (50495, 74)\n",
      "n shape (50495, 72)\n",
      "result s  (50495, 146)\n",
      "for the key XTZ/USDT\n",
      "old shape (45287, 74)\n",
      "n shape (45287, 72)\n",
      "result s  (45287, 146)\n",
      "for the key KEY/USDT\n",
      "old shape (45968, 74)\n",
      "n shape (45968, 72)\n",
      "result s  (45968, 146)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing the .npy files\n",
    "directory_path = \"data_deep_learning/series_data\"\n",
    "\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".npy\"):  \n",
    "        key = file_name.split('.')[0].replace(\"&\",\"/\")\n",
    "        print(\"for the key\" ,key)\n",
    "\n",
    "        data = np.load(directory_path+\"/\"+file_name)\n",
    "        aux_h = hist[key]\n",
    "        l_i = dic_s[key][\"lower_target_i\"]\n",
    "        u_i = dic_s[key][\"upper_target_i\"]\n",
    "        print(\"old shape\",data.shape)\n",
    "        print(\"n shape\", aux_h[l_i:u_i, :].shape)\n",
    "        n_arr = np.concatenate((aux_h[l_i:u_i, :], data), axis=1)\n",
    "        print(\"result s \",n_arr.shape)\n",
    "\n",
    "        \n",
    "        # Save the modified data back to the file\n",
    "        np.save(directory_path+\"/\"+file_name, n_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = 700000000\n",
    "min_key = \"nada\"\n",
    "for key in dic_s_i:\n",
    "    series_key = targets[:,dic_s_i[key]]\n",
    "    s_not_nans = sum(not np.isnan(x) for x in series_key)\n",
    "    if s_not_nans < mini:\n",
    "        mini = s_not_nans\n",
    "        min_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 8 9]\n",
      " [1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(np.array([arr[2,:], arr[0,:], arr[1,:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 2, 8, 7, 5, 0, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "l = [i for i in range(9)]\n",
    "random.shuffle(l)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower not nan ind 12774\n",
      "max not nan ind 38731\n"
     ]
    }
   ],
   "source": [
    "min_series = targets[:,dic_s_i[min_key]]\n",
    "for i, val in enumerate(min_series):\n",
    "    if not np.isnan(val):\n",
    "        print(f\"lower not nan ind {i}\")\n",
    "        break\n",
    "for i in range(len(min_series) -1, 0, -1):\n",
    "    if not np.isnan(min_series[i]):\n",
    "        print(f\"max not nan ind {i}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in min_series[12774:38732]:\n",
    "    if np.isnan(i):\n",
    "        print(\"an error!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_btc=df_hist['BTC/USDT']\n",
    "##index of interest possibly 'BTC/USDT'\n",
    "##from index  2376 on the matrices in hist_btc have all of those indices\n",
    "##there are targets from index 0 up to -30 the last 29 records have NaNs\n",
    "target_prices = df_prices['BTC/USDT']\n",
    "\n",
    "for i in range(df_prices.shape[1]):\n",
    "    if df_prices.columns[i] == \"BTC/USDT\":\n",
    "        print(\"Found it at\", i)\n",
    "        ind = i\n",
    "#the found index is ahead by 1 because the first col is 'Unamed: 0', is the ts\n",
    "targets = np.load(pre_path + 'targets', allow_pickle=True)\n",
    "target_btc=targets[:,ind -1]   \n",
    "\n",
    "#del df_hist\n",
    "del df_prices\n",
    "del targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find first index where there's no NaNs in its corresp matrix\n",
    "for ind, val in enumerate(btc_rets):\n",
    "    if not np.isnan(val.sum()):\n",
    "        found = ind\n",
    "        print(f\"There is something in index {ind}\")\n",
    "        break\n",
    "#check that the elems from there on have no NaNs    \n",
    "for i in range(found, len(btc_rets)):\n",
    "    if np.isnan(btc_rets[i].sum()):\n",
    "        print(\"There are missing elements!!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look starting at the end where are not NaNs in the target\n",
    "for i in range(1,10000):\n",
    "    if not np.isnan(target_btc[-i]):\n",
    "        ind2 = i\n",
    "        print(f\"Not an NaN at -{i}\")\n",
    "        break\n",
    "#filter data to start from step 2376 up to -30\n",
    "found = 2376\n",
    "#target_btc_f = target_btc[found:-ind2 + 1]\n",
    "target_prices_f = target_prices[found:-ind2 + 1]\n",
    "btc_rets_f = btc_rets[found:-ind2 + 1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61406, 145)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data.npy')\n",
    "ndata = np.array([np.concatenate((i, z)) for i, z in zip(data, btc_rets_f)])\n",
    "ndata.shape\n",
    "#np.save('with_rets_data.npy', ndata)\n",
    "#np.save('tarbet.npy', target_btc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/carlos/Documents/Peccala/deepL_tests'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_from_ts(data, seq_length):\n",
    "    \"\"\"\n",
    "    Function that returns the vectors with corresponding seq_length,\n",
    "    of observations in the past.\n",
    "    Output should have shape (data.shape[0] - seq_length, seq_length, num_features)\n",
    "    \n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        # Extract the sequence of features\n",
    "        sequence = data[i:i+seq_length]\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Example usage\n",
    "\n",
    "seq_length = 7\n",
    "\n",
    "data = np.array([np.concatenate(([i], z)) for i, z in zip(target_prices_f, hist_btc_f)])\n",
    "\n",
    "\n",
    "sequences = create_seq_from_ts(data, seq_length)\n",
    "print(\"seq shape\", sequences.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"seq shap\",sequences.shape)\n",
    "print(\"labels shape\", target_btc_f[seq_length-1:].shape)\n",
    "\n",
    "test_percentage = 0.13\n",
    "test_l = int(sequences.shape[0]*test_percentage)\n",
    "X_tr = torch.Tensor(sequences[:-test_l,:,:])\n",
    "y_tr = torch.Tensor(target_btc_f[seq_length-1:-test_l])\n",
    "X_tst = torch.Tensor(sequences[-test_l:,:,:])\n",
    "y_tst = torch.Tensor(target_btc_f[-test_l:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=128, num_heads=8, num_layers=4, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input Embedding\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_len=5000, d_model=d_model)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: shape (batch_size, seq_len, input_dim)\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        \n",
    "        # Embed input features\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        \n",
    "        # Permute for transformer: (seq_len, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Pass through the transformer encoder\n",
    "        x = self.encoder(x)  # shape: (seq_len, batch_size, d_model)\n",
    "        \n",
    "        # Aggregate sequence output (e.g., take the mean)\n",
    "        x = x.mean(dim=0)  # shape: (batch_size, d_model)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x)  # shape: (batch_size, num_classes)\n",
    "        return out\n",
    "    \n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        # Positional encoding following \"Attention is All You Need\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "# Parameters\n",
    "input_dim = 73  # 72 indicators + price\n",
    "num_classes = 2  # Example: 5 classes for classification\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Zero the gradientsj\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_tr)  # Shape: (batch_size, num_classes)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_tr)\n",
    "    \n",
    "    # Backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step (update model parameters)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress\n",
    "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm_mts_with_window(data, window_size=30):\n",
    "    \"\"\"\n",
    "    Normalize a multivariate time series so that each component of the vector has\n",
    "    a standard deviation of 1 within a sliding window lokking back in time.\n",
    "    If window size is 1 does nothing.\n",
    "\n",
    "    Returns np.array normalized time series data with the same shape as input.\n",
    "    \"\"\"\n",
    "    t_steps, _ = data.shape\n",
    "    normalized_data = np.zeros_like(data)\n",
    "\n",
    "    for t in range(t_steps):\n",
    "        start_i = max(0, t - window_size + 1)\n",
    "        end_i = t + 1\n",
    "        window = data[start_i:end_i]\n",
    "        std = np.std(window, axis=0)\n",
    "        mean = np.mean(window, axis=0)\n",
    "        std[std == 0] = 1\n",
    "        normalized_data[t] = (data[t])/ std\n",
    "\n",
    "    return normalized_data\n",
    "def create_seq_from_ts(data, seq_length):\n",
    "    \"\"\"\n",
    "    Function that returns the vectors with corresponding seq_length,\n",
    "    of observations in the past.\n",
    "\n",
    "    Output should have shape (data.shape[0] - seq_length, seq_length, num_features)\n",
    "\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        # Extract the sequence of features\n",
    "        sequence = data[i:i+seq_length]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something wrong at 12\n",
      " data tr x shape : (11780, 30, 145)\n",
      " data tr y shape : (11780,)\n",
      " data val x shape : (2940, 30, 145)\n",
      " data val y shape : (2940,)\n",
      " data sts x shape : (14700, 30, 145)\n",
      " data tst y shape : (14700,)\n",
      " \n",
      " shuffled x data shape  torch.Size([11780, 30, 145])\n",
      " \n",
      " shuffled y data shape  torch.Size([11780])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "seq_length = 30\n",
    "val_perc = 0.1\n",
    "test_perc = 0.5\n",
    "data = np.load(\"few_series/np_series_concat2.npy\")\n",
    "def prepare_data(data):\n",
    "    '''This function put the data into three different arrays,\n",
    "    respectively train, validation and test. Keeping the Easliest data for testing and \n",
    "    the further in time data for training.\n",
    "    retuns: (data_training(x,y), data_validation(x,y), data_testing(x,y))\n",
    "    '''\n",
    "    data_tr_x, data_val_x, data_tst_x = [], [], [] \n",
    "    data_tr_y, data_val_y, data_tst_y = [], [], []\n",
    "    \n",
    "    for j in range(data.shape[0]):  \n",
    "        xdata = data[j, -1500:,:] #take only the last 1500 measurements\n",
    "        data_2_norm = xdata[:, :-1]\n",
    "\n",
    "        # Check for NaNs\n",
    "        if np.isnan(data_2_norm).any().any():\n",
    "            print(f\"Something wrong at {j}\")\n",
    "            continue\n",
    "        \n",
    "        data_2_norm = norm_mts_with_window(data_2_norm)\n",
    "\n",
    "        # Create sequences\n",
    "        sequences = create_seq_from_ts(data_2_norm, seq_length)\n",
    "        if np.isnan(sequences).any().any():\n",
    "            print(f\"Something wrong at sequences in {j}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare targets\n",
    "        y = xdata[seq_length - 1:, -1]\n",
    "\n",
    "        # Split into training, validation, and testing\n",
    "        val_i = int(sequences.shape[0] * val_perc)\n",
    "        test_i = int(sequences.shape[0] * test_perc)\n",
    "        X_tr, X_val, X_tst = sequences[:-(val_i + test_i), :, :], sequences[-(val_i + test_i):-test_i, :, :], sequences[-test_i:, :, :]\n",
    "        y_tr, y_val, y_tst = y[:-(val_i + test_i)], y[-(val_i + test_i):-test_i], y[-test_i:]\n",
    "        y_val = (y_val + 1) / 2 \n",
    "        y_tr = (y_tr + 1) / 2\n",
    "        y_tst = (y_tst + 1) / 2  \n",
    "        n_pos_tr = y_tr.sum()\n",
    "        weight = (len(y_tr) -n_pos_tr)/n_pos_tr\n",
    "        data_tr_x.append(X_tr)\n",
    "        data_val_x.append(X_val)\n",
    "        data_tst_x.append(X_tst)\n",
    "        data_tr_y.append(y_tr)\n",
    "        data_val_y.append(y_val)\n",
    "        data_tst_y.append(y_tst)\n",
    "    data_tr_x = np.concatenate(data_tr_x, axis=0)\n",
    "    data_val_x = np.concatenate(data_val_x, axis=0) \n",
    "    data_tst_x = np.concatenate(data_tst_x, axis=0) \n",
    "    data_tr_y = np.concatenate(data_tr_y, axis=0)\n",
    "    data_val_y = np.concatenate(data_val_y, axis=0) \n",
    "    data_tst_y = np.concatenate(data_tst_y, axis=0)  \n",
    "    return ((data_tr_x, data_tr_y), \n",
    "            (data_val_x, data_val_y),\n",
    "            (data_tst_x, data_tst_y))\n",
    "    \n",
    "def shuffle_data(X_data, y_data):\n",
    "    \"\"\" With an input tensor (X,y) returns the tensor keeping X-y relations\n",
    "        with the indices shuffled\n",
    "    \"\"\"     \n",
    "    n_samples = X_data.size(0)\n",
    "    indices = torch.randperm(n_samples)\n",
    "    X_shuffled = X_data[indices]\n",
    "    y_shuffled = y_data[indices]\n",
    "    return X_shuffled, y_shuffled\n",
    "data_res = prepare_data(data)   \n",
    "print(\" data tr x shape :\", data_res[0][0].shape)\n",
    "print(\" data tr y shape :\", data_res[0][1].shape)\n",
    "\n",
    "print(\" data val x shape :\", data_res[1][0].shape)\n",
    "print(\" data val y shape :\", data_res[1][1].shape)\n",
    "print(\" data sts x shape :\", data_res[2][0].shape)\n",
    "print(\" data tst y shape :\", data_res[2][1].shape)\n",
    "X_tr_tensor = torch.Tensor(data_res[0][0])\n",
    "y_tr_tensor = torch.Tensor(data_res[0][1])\n",
    "X_shuffled, y_shuffled = shuffle_data(X_tr_tensor, y_tr_tensor)  \n",
    "print(\" \\n shuffled x data shape \", X_shuffled.shape)\n",
    "print(\" \\n shuffled y data shape \", y_shuffled.shape) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5275891341256367"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(y_shuffled).item() / y_shuffled.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (non-zero coefficients): [   0    1    2 ... 4347 4348 4349]\n",
      "Number of selected features: 4219\n",
      "Selected time series indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144]\n",
      "Number of selected time series: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data: (n_samples, n_series, seq_len)\n",
    "n_samples = 29420 # Number of samples\n",
    "n_series = 145    # Number of time series per sample\n",
    "seq_len = 30      # Length of each time series\n",
    "\n",
    "# Simulated multivariate time series data\n",
    "       # Binary labels\n",
    "\n",
    "# Step 1: Flatten each sample (n_samples, n_series * seq_len)\n",
    "X_flat = x_data.reshape(n_samples, -1)\n",
    "\n",
    "# Step 2: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_flat = scaler.fit_transform(X_flat)\n",
    "\n",
    "# Step 3: Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_flat, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train L1-regularized Logistic Regression\n",
    "lasso_logistic = LogisticRegression(\n",
    "    penalty='l1', \n",
    "    solver='saga',  # 'saga' supports L1 regularization\n",
    "    C=1.0,          # Regularization strength (inverse of lambda, tune as needed)\n",
    "    max_iter=1000\n",
    ")\n",
    "lasso_logistic.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Inspect coefficients\n",
    "coefficients = lasso_logistic.coef_[0]  # Coefficients for the first (binary) class\n",
    "\n",
    "# Identify non-zero coefficients\n",
    "selected_features = np.where(coefficients != 0)[0]\n",
    "print(f\"Selected features (non-zero coefficients): {selected_features}\")\n",
    "print(f\"Number of selected features: {len(selected_features)}\")\n",
    "\n",
    "# Step 6: Map selected features back to time series\n",
    "selected_series = np.unique(selected_features // seq_len)\n",
    "print(f\"Selected time series indices: {selected_series}\")\n",
    "print(f\"Number of selected time series: {len(selected_series)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features = [v[0] for v in scores_dic.items() if v[1]>0.005]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something wrong at 12\n",
      "resulting x shape (29420, 30, 145)\n",
      "resulting y shape (29420,)\n"
     ]
    }
   ],
   "source": [
    "##cell to add the normalized data into a single np array of shape (n_samples, 73)\n",
    "seq_length = 30\n",
    "import numpy as np\n",
    "def norm_mts_with_window(data, window_size=30):\n",
    "    \"\"\"\n",
    "    Normalize a multivariate time series so that each component of the vector has\n",
    "    a standard deviation of 1 within a sliding window lokking back in time.\n",
    "    If window size is 1 does nothing.\n",
    "\n",
    "    Returns np.array normalized time series data with the same shape as input.\n",
    "    \"\"\"\n",
    "    t_steps, _ = data.shape\n",
    "    normalized_data = np.zeros_like(data)\n",
    "\n",
    "    for t in range(t_steps):\n",
    "        start_i = max(0, t - window_size + 1)\n",
    "        end_i = t + 1\n",
    "        window = data[start_i:end_i]\n",
    "        std = np.std(window, axis=0)\n",
    "        mean = np.mean(window, axis=0)\n",
    "        std[std == 0] = 1\n",
    "        normalized_data[t] = (data[t])/ std\n",
    "\n",
    "    return normalized_data\n",
    "series=data #np.load('/Users/carlos/Documents/Peccala/deepL_tests/few_series/np_series_concat2.npy')\n",
    "x_data = []\n",
    "y_data = []\n",
    "for i in range(series.shape[0]):\n",
    "    xdata= series[i, -1500:,:] #restrict to last 3000 measurements, do not include histograms, first\n",
    "    #histograms are the first 72 features\n",
    "    data_2_norm = xdata[: ,: -1]\n",
    "\n",
    "    if np.isnan(data_2_norm).any().any():\n",
    "        print(f\"Something wrong at {i}\")\n",
    "        continue\n",
    "    data_2_norm = norm_mts_with_window(data_2_norm)\n",
    "    sequences = create_seq_from_ts(data_2_norm, seq_length)\n",
    "    x_data.append(sequences)\n",
    "    y_data.append( xdata[seq_length - 1:, -1])\n",
    "x_data = np.concatenate(x_data, axis=0)    \n",
    "y_data = np.concatenate(y_data, axis=0)   \n",
    "print(f\"resulting x shape {x_data.shape}\" )    \n",
    "print(f\"resulting y shape {y_data.shape}\" )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "series=np.load('/Users/carlos/Documents/Peccala/deepL_tests/few_series/np_series_concat2.npy')\n",
    "x_data = []\n",
    "y_data = []\n",
    "for i in range(series.shape[0]):\n",
    "    xdata= series[i, -3000:,72:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "x=create_seq_from_ts(xdata, seq_length=3)\n",
    "print(np.isnan(x).any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for tr sum -1338.0 of 40800 samples \n",
      " for tst sum 230.0 of 4800 samples \n"
     ]
    }
   ],
   "source": [
    "tst_i=int(y_data.shape[0]*0.15)\n",
    "tst_i2 = int(y_data.shape[0]*0.05)\n",
    "tr_x =x_data[:-tst_i]\n",
    "tst_x=x_data[-tst_i:-tst_i2]\n",
    "tr_y = y_data[:-tst_i]\n",
    "tst_y= y_data[-tst_i:-tst_i2]\n",
    "print(f\" for tr sum {sum(tr_y)} of {len(tr_y)} samples \")\n",
    "print(f\" for tst sum {sum(tst_y)} of {len(tst_y)} samples \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages (from imbalanced-learn) (1.6.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/Users/carlos/miniconda3/envs/timeseries/lib/python3.10/site-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(tr_x, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.9108953283884931 max corr at 15\n",
      " 0.9297958729793113 max corr at 17\n",
      " 0.961457710222429 max corr at 19\n",
      " 0.9606166467335108 max corr at 20\n",
      " 0.9299131427871605 max corr at 21\n",
      " 0.9223859242882432 max corr at 22\n",
      " 0.9351430421910194 max corr at 23\n",
      " 0.9389277325163841 max corr at 24\n",
      " 0.9658937041107518 max corr at 25\n",
      " 0.9297472796672076 max corr at 26\n",
      " 0.9151739734110896 max corr at 27\n",
      " 0.8809021727584668 max corr at 28\n",
      " 0.9678563677641636 max corr at 31\n",
      " 0.9565679942440866 max corr at 32\n",
      " 0.9284400874443614 max corr at 33\n",
      " 0.9416147684493164 max corr at 34\n",
      " 0.9541464179740394 max corr at 35\n",
      " 0.9361678945548246 max corr at 36\n",
      " 0.9387799029504377 max corr at 37\n",
      " 0.8915745310356116 max corr at 38\n",
      " 0.9271048847729666 max corr at 42\n",
      " 0.9313561657965201 max corr at 43\n",
      " 0.9655884837392488 max corr at 44\n",
      " 0.9767924619733681 max corr at 45\n",
      " 0.9690411846960109 max corr at 46\n",
      " 0.9064103296264922 max corr at 47\n",
      " 0.9359281740376326 max corr at 49\n",
      " 0.946175821527798 max corr at 50\n",
      " 0.9569045257280056 max corr at 51\n",
      " 0.9360476310161178 max corr at 52\n",
      " 0.9613037686993992 max corr at 54\n",
      " 0.9267534623350898 max corr at 55\n",
      " 0.9209624399241383 max corr at 56\n",
      " 0.9058387480905982 max corr at 59\n",
      " 0.8955722498092561 max corr at 60\n",
      " 0.8989222327695794 max corr at 61\n",
      " 0.9572239383676638 max corr at 62\n",
      " 0.937769805775072 max corr at 64\n",
      " 0.8824103799498084 max corr at 65\n",
      " 0.9516211973399041 max corr at 67\n",
      " 0.9466239845120277 max corr at 68\n",
      " 0.9473816499117288 max corr at 69\n",
      " 0.93728205680798 max corr at 70\n",
      "final count  43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#df = pd.DataFrame(x_data)\n",
    "\n",
    "#correlation_matrix = df.corr()\n",
    "\n",
    "#print(\"Correlation Matrix:\")\n",
    "count = 0\n",
    "for i in range(len(correlation_matrix)-1):\n",
    "    c = max(correlation_matrix[i][i+1:])\n",
    "    if c > 0.88:\n",
    "        print(f\" {c} max corr at {i}\")\n",
    "        count +=1\n",
    "print(\"final count \", count)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 3 3]\n",
      "  [2 2 3]]\n",
      "\n",
      " [[2 2 3]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[3 3 3]\n",
      "  [1 0 3]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def norm_mts_with_window(data, window_size=30):\n",
    "    \"\"\"\n",
    "    Normalize a multivariate time series so that each component of the vector has\n",
    "    a standard deviation of 1 within a sliding window lokking back in time.\n",
    "    If window size is 1 does nothing.\n",
    "\n",
    "    Returns np.array normalized time series data with the same shape as input.\n",
    "    \"\"\"\n",
    "    t_steps, _ = data.shape\n",
    "    normalized_data = np.zeros_like(data)\n",
    "\n",
    "    for t in range(t_steps):\n",
    "        start_i = max(0, t - window_size + 1)\n",
    "        end_i = t + 1\n",
    "        window = data[start_i:end_i]\n",
    "        std = np.std(window, axis=0)\n",
    "        mean = np.mean(window, axis=0)\n",
    "        std[std == 0] = 1\n",
    "        normalized_data[t] = (data[t])/ std\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "#function to create tensor with seg_length of backward steps\n",
    "def create_seq_from_ts(data, seq_length):\n",
    "    \"\"\"\n",
    "    Function that returns the vectors with corresponding seq_length,\n",
    "    of observations in the past.\n",
    "\n",
    "    Output should have shape (data.shape[0] - seq_length, seq_length, num_features)\n",
    "\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        # Extract the sequence of features\n",
    "        sequence = data[i:i+seq_length]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    return sequences\n",
    "    \n",
    "\n",
    "x=np.array([[1,3,3], [2,2,3], [3,3,3],[1,0,3]])\n",
    "y=create_seq_from_ts(x, seq_length=2) \n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([1,0,1])\n",
    "print(a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,4]\n",
    "a.pop(2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skfda.representation.basis import FourierBasis\n",
    "from skfda.preprocessing.dim_reduction.feature_extraction import FPCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example: multivariate time series\n",
    "# data: (n_samples, n_timepoints, n_features)\n",
    "n_samples, n_timepoints, n_features = 100, 50, 3\n",
    "data = np.random.randn(n_samples, n_timepoints, n_features)\n",
    "labels = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "# Transform each feature into functional data\n",
    "basis = FourierBasis(n_basis=10, domain_range=(0, n_timepoints))\n",
    "fd_list = [basis.fit_transform(data[:, :, i]) for i in range(n_features)]\n",
    "\n",
    "# Combine into multivariate functional data\n",
    "from skfda.representation.grid import FDataGrid\n",
    "combined_fd = FDataGrid(data_matrix=np.stack(fd_list, axis=-1))\n",
    "\n",
    "# Apply fMPCA\n",
    "fpca = FPCA(n_components=5)  # Retain 5 components\n",
    "reduced_data = fpca.fit_transform(combined_fd)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Classification accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55.847100745059926, 56.85131195335277, 53.806284418529316, 53.48234531908001, 62.45545837382572, 60.349854227405245, 57.142857142857146, 54.81049562682216, 57.33722060252673, 54.84288953676709, 58.082280531260125, 58.63297700032394, 57.24003887269193, 57.175251052802075, 52.445740200842245, 58.73015873015873, 57.07806932296728]\n"
     ]
    }
   ],
   "source": [
    "aux=txt.split('val acc')[1:]\n",
    "print([float(i.split(',')[0].replace(\":\",'')) for i in aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [1, 2, 3]}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(str({1:[1,2,3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
